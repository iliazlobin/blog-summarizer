[
    {
        "title": "Medical Text Processing on Google Cloud",
        "url": "https://cloud.google.com/blog/topics/healthcare-life-sciences/medical-text-processing-on-google-cloud",
        "authors": [
            {
                "name": "Alex Burdenko",
                "jobTitle": "Customer Engineer - Data, Analytics and ML Specialist"
            },
            {
                "name": "Joan Kallogjeri",
                "jobTitle": "Customer Engineer - Data, Analytics and ML Specialist"
            }
        ],
        "date": "2024-02-08T00:00:00.000Z",
        "tag": "Healthcare & Life Sciences",
        "text": "The FDA has a history of using real world evidence (RWE) as an integral component of the drug approval process. Moreover, RWE can mitigate the need for placebos in some clinical trials. The clinical records that make RWE evidence useful, however, often reside in unstructured formats, such as doctor\u2019s notes, and must be \u201cabstracted\u201d into a clinical structured format. Cloud technologies and AI can help accelerate this process, making it significantly faster and more scalable.Leading drug researchers are starting to augment their clinical trials with real world data for their FDA study submissions because it saves time and is more cost effective. Once the patient\u2019s care concludes, the vast amounts of historical unstructured patient medical data ends up being a contributor to increasing storage needs. Unstructured data is key and critical in clinical decision support systems. In their original unstructured format, insights need a human to review the unstructured data. With no discrete data points from which insights can be quickly drawn, unstructured medical data can result in increased care gaps and care variances. Simple logic dictates that unassisted human abstraction alone is not fast or accurate enough to abstract all of this patient data. Applied natural language processing (NLP) using serverless software components on Google Cloud provides an efficient way of identifying and guiding clinical abstractors towards a prioritized list of patient medical documents.How to run Medical Text Processing on Google CloudUsing Google Cloud\u2019s Vertex Workbench Jupyter Notebooks, you can create a data pipeline that takes raw clinical text documents and processes them through Google Cloud\u2019s Healthcare Natural Language API landing the structured json output into BigQuery. From there, you can build a dashboard that can show clinical text characteristics, e.g., number of labels and relationships. From this, you\u2019ll be able to build a trainable language model that can extract text and be further improved over time by human labeling.To better understand how the solution addresses these challenges, let\u2019s review the medical text entity extraction workflow:Document AI for Data Ingestion. The system starts with a PDF file that contains de-identified medical text, such as a doctor\u2019s hand-written notes or other unstructured text. This unstructured data is first processed by Document AI using optical character recognition (OCR) technology to digitize the text and images.Natural Language Processing. The Cloud Natural Language API includes a set of pretrained models, including models for extracting and classifying medical text. The labels that are generated as part of the output of this service will serve as the \u201cground truth\u201d labels for the Vertex AI AutoML service where additional, domain specific custom labels will be added.Vertex AI AutoML. Vertex AI AutoML offers a machine learning toolset for human-in-the-loop dataset labeling and automatic label classification, using a Google model that your team can train with your data, even if team members possess little coding or data science expertise.BigQuery Tables. NLP processed records are stored in BigQuery for further processing and visualization.Looker Dashboard. The Looker Dashboard acts as the central \u201cbrain\u201d for the clinical text abstraction process by serving visualizations that help the team identify the highest priority clinical documents using metrics like tag and concept \u201cdensity.\u201dPython Jupyter Notebook. Use either Colab (free) or Vertex AI (enterprise) notebooks to explore your text data and call different APIs for ingestion and NLP.The Healthcare Natural Language APIThe Healthcare Natural Language API lets you efficiently run medical text entity resolution at scale by focusing on the following optimizations:Optimizing document OCR and data extraction by using scalable Cloud Functions to run the document processing in parallel.Optimizing cost and time to market by using completely serverless and managed services.Facilitating a flexible and inclusive workflow that incorporates human-in-the-loop abstraction assisted by ML.The following diagram shows the architecture of the solution.A set of reusable Python scripts that can be run from either a Jupyter notebook or Google Cloud Functions that drive the various stages of an NLP processing pipeline, which converts medical text to structured patient data and a Looker dashboard that acts as the decision support interface for teams of human clinical abstractors.A set of Google Cloud Storage Buckets to support the various stages of data processing (illustrated below).Two BigQuery tables, called \u201cEntity\u201d and \u201cDocument,\u201d in a dataset called \u201centity,\u201d are created as the data model for the Looker dashboard.A Vertex AI dataset used for human-in-the-loop labeling by clinical abstractors and to send labeling requests to the Google Vertex AI Labeling Team for added flexibility and scale.A Looker dashboard that displays the stack-ranked documents to be processed in order by the human abstractors based on a custom \u201cdensity\u201d metric, which is the number of data elements (labels) found in those documents. This dashboard will guide the human abstractors to look at the sparsely labeled documents first and let Google\u2019s NLP do the heavy lifting.A list of documents, by density score, helps human abstractors know which documents need a lot of work versus only a light review.This Look (view) shows the coded medical text that was mapped to the UMLS clinical ontology by the Google Healthcare Natural Language API.This Look (view) shows the entity mentions, including the subject of each mention and its confidence score, allowing for loading into a biomedical knowledge graph for further downstream analysis.This Look (view) shows the entity mentions found in the raw document text.Future Topics and Next StepsThis demo loaded the entity and document metadata into BigQuery and Looker but didn\u2019t load the rich relationships that come out-of-the-box from the Healthcare Natural Language API. Using those relationships, it is possible to create a biomedical knowledge graph and explore the pathways between disease, treatment, and cohorts, and to help generate new hypotheses linking these facts.We created a barebones dashboard with Looker. However, Looker has rich functionality, such as the ability to push to channels like chat when a document is available for review or to visualize the patient as a medical knowledge graph of related entities or embedding ML predictions right in the Looker LookML itself. This dashboard should be considered just a starting point for Looker powered clinical informatics.To learn more about the Healthcare Natural Language API, please visit our product page. To try it yourself for free, please visit this demo link.For help with loading this example medical text into a Vertex AI dataset for labeling, please contact the Google Cloud Biotech Team.Data PrivacyNo real patient data was used for any part of this blog post. Google Cloud\u2019s customers retain control over their data. In healthcare settings, access and use of patient data is protected through the implementation of Google Cloud\u2019s reliable infrastructure and secure data storage that support HIPAA compliance, along with each customer\u2019s security, privacy controls, and processes. To learn more about data privacy on Google Cloud, check out this link.Posted inHealthcare & Life SciencesAI & Machine Learning"
    },
    {
        "title": "No GPU? No problem. localllm lets you develop gen AI apps on local CPUs",
        "url": "https://cloud.google.com/blog/products/application-development/new-localllm-lets-you-develop-gen-ai-apps-locally-without-gpus",
        "authors": [
            {
                "name": "Geoffrey Anderson",
                "jobTitle": "Product Manager, Google Cloud"
            },
            {
                "name": "Christie Warwick",
                "jobTitle": "Software Engineer, Google Cloud"
            }
        ],
        "date": "2024-02-07T00:00:00.000Z",
        "tag": "Application Development",
        "text": "In today's fast-paced AI landscape, developers face numerous challenges when it comes to building applications that use large language models (LLMs). In particular, the scarcity of GPUs, which are traditionally required for running LLMs, poses a significant hurdle. In this post, we introduce you to a novel solution that allows developers to harness the power of LLMs locally on CPU and memory, right within Cloud Workstations, Google Cloud\u2019s fully managed development environment. The models we use in this walkthrough are located on Hugging Face and are specifically in a repo from \u201cThe Bloke\u201d and are compatible with the quantization method used to allow them to run on CPUs or low power GPUs. This innovative approach not only eliminates the need for GPUs but also opens up a world of possibilities for seamless and efficient application development. By using a combination of \u201cquantized models,\u201d Cloud Workstations, a new open-source tool named localllm, and generally available resources, you can develop AI-based applications on a well-equipped development workstation, leveraging existing processes and workflows.\nQuantized models + Cloud Workstations == Productivity\nQuantized models are AI models that have been optimized to run on local devices with limited computational resources. These models are designed to be more efficient in terms of memory usage and processing power, allowing them to run smoothly on devices such as smartphones, laptops, and other edge devices. In this case, we are running them on Cloud Workstations with ample available resources. Here are some great examples of why leveraging quantized models in your development loop may unblock your efforts:\n\n\nImproved performance: Quantized models are optimized to perform computations using lower-precision data types such as 8-bit integers, instead of standard 32-bit floating-point numbers. This reduction in precision allows for faster computations and improved performance on devices with limited resources.\n\n\nReduced memory footprint: Quantization techniques help reduce the memory requirements of AI models. By representing weights and activations with fewer bits, the overall size of the model is reduced, making it easier to fit on devices with limited storage capacity.\u00a0\n\n\nFaster inference: Quantized models can perform computations more quickly due to their reduced precision and smaller model size. This enables faster inference times, allowing AI applications to run more smoothly and responsively on local devices.\n\n\nCombining quantized models with Cloud Workstations allows you to take advantage of the flexibility, scalability and cost effectiveness of Cloud Workstations. Moreover, the traditional approach of relying on remote servers or cloud-based GPU instances for LLM-based application development can introduce latency, security concerns, and dependency on third-party services. A solution that lets you leverage LLMs locally, within your Cloud Workstations, without compromising performance, security, or control over your data, can have a lot of benefits.\nIntroducing localllm\nToday, we\u2019re introducing\u00a0 localllm, a set of tools and libraries that provides easy access to quantized models from HuggingFace through a command-line utility. localllm can be a game-changer for developers seeking to leverage LLMs without the constraints of GPU availability. This repository provides a comprehensive framework and tools to run LLMs locally on CPU and memory, right within the Google Cloud Workstation, using this method (though you can also run LLM models on your local machine or anywhere with sufficient CPU). By eliminating the dependency on GPUs, you can unlock the full potential of LLMs for your application development needs.\nKey features and benefits\nGPU-free LLM execution: localllm lets you execute LLMs on CPU and memory, removing the need for scarce GPU resources, so you can integrate LLMs into your application development workflows, without compromising performance or productivity.\nEnhanced productivity: With localllm, you use LLMs directly within the Google Cloud ecosystem. This integration streamlines the development process, reducing the complexities associated with remote server setups or reliance on external services. Now, you can focus on building innovative applications without managing GPUs.\nCost efficiency: By leveraging localllm, you can significantly reduce infrastructure costs associated with GPU provisioning. The ability to run LLMs on CPU and memory within the Google Cloud environment lets you optimize resource utilization, resulting in cost savings and improved return on investment.\nImproved data security: Running LLMs locally on CPU and memory helps keep sensitive data within your control. With localllm, you can mitigate the risks associated with data transfer and third-party access, enhancing data security and privacy.\nSeamless integration with Google Cloud services: localllm integrates with various Google Cloud services, including data storage, machine learning APIs, or other Google Cloud services, so you can leverage the full potential of the Google Cloud ecosystem.\u00a0\nGetting started with localllm\nTo get started with the localllm, visit the GitHub repository at https://github.com/googlecloudplatform/localllm. The repository provides detailed documentation, code samples, and step-by-step instructions to set up and utilize LLMs locally on CPU and memory within the Google Cloud environment. You can explore the repository, contribute to its development, and leverage its capabilities to enhance your application development workflows.\u00a0\nOnce you\u2019ve cloned the repo locally, the following simple steps will run localllm with a quantized model of your choice from the HuggingFace repo \u201cThe Bloke,\u201d then execute an initial sample prompt query. For example we are using Llama.Loading...# Install the tools\npip3 install openai\npip3 install ./llm-tool/.\n\n# Download and run a model\nllm run TheBloke/Llama-2-13B-Ensemble-v5-GGUF 8000\n\n# Try out a query\n./querylocal.pyCreating a localllm-enabled Cloud Workstation\nTo get started with localllm and Cloud Workstations, you'll need a Google Cloud Project and to install the gcloud CLI. First, build a Cloud Workstations container that includes localllm, then use that as the basis for our developer workstation (which also comes pre-equipped with VS Code).Loading...gcloud config set project $PROJECT_ID\n\n# Enable needed services\ngcloud services enable \\\n  cloudbuild.googleapis.com \\\n  workstations.googleapis.com \\\n  container.googleapis.com \\\n  containeranalysis.googleapis.com \\\n  containerscanning.googleapis.com \\\n  artifactregistry.googleapis.com\n\n# Create AR Docker repository\ngcloud artifacts repositories create localllm \\\n  --location=us-central1 \\\n  --repository-format=dockerNext, submit a build of the Dockerfile, which also pushes the image to Artifact Registry.Loading...gcloud builds submit .The published image is namedLoading...us-central1-docker.pkg.dev/$PROJECT_ID/localllm/localllm.The next step is to create and launch a workstation using our custom image. We suggest using a machine type of e2-standard-32 (32 vCPU, 16 core and 128 GB memory), an admittedly beefy machine.\nThe following example uses gcloud to configure a cluster, configuration and workstation using our custom base image with llm installed. Replace $CLUSTER with your desired cluster name, and the command below will create a new one (which takes ~20 minutes).Loading...gcloud workstations clusters create $CLUSTER \\\n  --region=us-central1The next steps create the workstation, and starts it up. These steps will take ~10 minutes to run.Loading...# Create workstation configuration\ngcloud workstations configs create localllm-workstation \\\n  --region=us-central1 \\\n  --cluster=$CLUSTER \\\n  --machine-type=e2-standard-32 \\\n  --container-custom-image=us-central1-docker.pkg.dev/$PROJECT_ID/localllm/localllm\n\n# Create the workstation\ngcloud workstations create localllm-workstation \\\n  --cluster=$CLUSTER \\\n  --config=localllm-workstation \\\n  --region=us-central1\n\n# Grant access to the default Cloud Workstation Service Account\ngcloud artifacts repositories add-iam-policy-binding \\\n  localllm \\\n  --location=us-central1 \\\n  --member=serviceAccount:service-$PROJECT_NUM@gcp-sa-workstationsvm.iam.gserviceaccount.com \\\n  --role=roles/artifactregistry.reader\n\n# Start the workstation\ngcloud workstations start localllm-workstation \\\n  --cluster=$CLUSTER \\\n  --config=localllm-workstation \\\n  --region=us-central1You can connect to the workstation using ssh (shown below), or interactively in the browser.Loading...gcloud workstations ssh localllm-workstation \\\n  --cluster=$CLUSTER \\\n  --config=localllm-workstation \\\n  --region=us-central1After serving a model (via the llm run command with the port of your choice), you can interact with the model by visiting the live OpenAPI documentation page. You can apply this process to any model listed in the Bloke\u2019s repo on HuggingFace Lllama was used in this scenario as an example. First, get the hostname of the workstation using:Loading...gcloud workstations describe localllm-workstation \\\n  --cluster=$CLUSTER \\\n  --config=localllm-workstation \\\n  --region=us-central1Then, in the browser, visit https://$PORT-$HOSTNAME/docs.\nConclusion\nlocalllm combined with Cloud Workstations revolutionizes AI-driven application development by letting you use LLMs locally on CPU and memory within the Google Cloud environment. By eliminating the need for GPUs, you can overcome the challenges posed by GPU scarcity and unlock the full potential of LLMs. With enhanced productivity, cost efficiency, and improved data security, localllm lets you build innovative applications with ease. Embrace the power of local LLMs and explore the possibilities within the Google Cloud ecosystem with localllm and Cloud Workstations today!Posted inApplication DevelopmentAI & Machine LearningComputeDevelopers & Practitioners"
    }
]